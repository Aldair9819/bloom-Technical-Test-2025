{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46063f9",
   "metadata": {},
   "source": [
    "# Conexiones con Ollama mediante su API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657798d5",
   "metadata": {},
   "source": [
    "## En esta Notebook se muestra la forma de obtener conexión con modelos LLM corriendo de manera local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7af83b",
   "metadata": {},
   "source": [
    "Documentación: https://github.com/ollama/ollama/blob/main/docs/api.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c45188",
   "metadata": {},
   "source": [
    "## Importamos las librerías.\n",
    "- Request para hacer peticiones a una API\n",
    "- dotenv para trabajar con archivos .env\n",
    "- os para poder obtener las variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93164ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "URL_OLLAMA = os.getenv(\"URL_OLLAMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0743e1",
   "metadata": {},
   "source": [
    "## Hacer un pull de un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5905d7f",
   "metadata": {},
   "source": [
    "Lista de modelos disponible: https://ollama.com/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f45a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull(model):\n",
    "    #URL por defecto para descargas de modelos LLM\n",
    "    url = f'{URL_OLLAMA}/api/pull'\n",
    "    # Comprobar que los valores obligatorios están presentes\n",
    "    response_data = {\n",
    "        \"model\": model,\n",
    "        \"stream\": False #Evita usar streaming. Que no mande la información en partes y la mande en su lugar todo en conjunto\n",
    "    }\n",
    "    try:\n",
    "        #Usa POST para enviar los datos al servidor\n",
    "        response = requests.post(url, json=response_data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json(), 200\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code, response.text)\n",
    "            return {\"error\": f\"Error {response.status_code}: {response.text}\"}, response.status_code\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return {\"error\": f\"An error occurred: {e}\"}, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f4c9052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'status': 'success'}, 200)\n"
     ]
    }
   ],
   "source": [
    "model_pull = \"gemma3:4b\"\n",
    "\n",
    "response = pull(model_pull)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61065698",
   "metadata": {},
   "source": [
    "## Ver los modelos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db94de20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': [{'name': 'gemma3:4b', 'model': 'gemma3:4b', 'modified_at': '2025-06-19T21:57:02.3379454Z', 'size': 3338801804, 'digest': 'a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma3', 'families': ['gemma3'], 'parameter_size': '4.3B', 'quantization_level': 'Q4_K_M'}}, {'name': 'llama3.2:latest', 'model': 'llama3.2:latest', 'modified_at': '2025-06-19T21:15:35.4544573Z', 'size': 2019393189, 'digest': 'a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '3.2B', 'quantization_level': 'Q4_K_M'}}]}\n",
      "Modelos:\n",
      "gemma3:4b\n",
      "--\n",
      "llama3.2:latest\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "url_tags = f\"{URL_OLLAMA}/api/tags\"\n",
    "#Usa GET para obtener información de todos los modelos locales\n",
    "response = requests.get(url_tags)\n",
    "if response.status_code == 200:\n",
    "    tags = response.json()\n",
    "    print(tags)\n",
    "    print(\"Modelos:\")\n",
    "    modelos = tags.get(\"models\", [])\n",
    "    for modelo in modelos:\n",
    "        print(modelo['name'])\n",
    "        print(\"--\")\n",
    "else:\n",
    "    print(\"Error al obtener los modelos:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61073605",
   "metadata": {},
   "source": [
    "## Uso de generación de texto mediante LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1a54e",
   "metadata": {},
   "source": [
    "- Model es el modelo a utilizar (Obligatorio)\n",
    "- Prompt es el texto que se le envía al modelo para generar una respuesta (opcional. Puede requerir solo activar el modelo sin prompt)\n",
    "- Format es el formato de salida que se desee (opcional) - Usa \"json\" o un JSON Schema\n",
    "- Keep_alive permite hacer que el modelo perdure en memoria los minutos que se requiera. 0 minutos libera memoria y -1 minutos lo hace para siempre (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,prompt = None, format = None, keep_alive = None):\n",
    "    url = f'{URL_OLLAMA}/api/generate'\n",
    "    # Comprobar que los valores obligatorios están presentes\n",
    "    response_data = {\n",
    "        \"model\": model,\n",
    "        \"stream\": False #Evita usar streaming. Que no mande la información en partes y la mande en su lugar todo en conjunto\n",
    "    }\n",
    "    #Corroborar los valores opcionales y los añade en dado caso de existir\n",
    "    if prompt:\n",
    "        response_data[\"prompt\"] = prompt\n",
    "    if format:\n",
    "        response_data[\"format\"] = format\n",
    "    if keep_alive:\n",
    "        response_data[\"keep_alive\"] = keep_alive\n",
    "    try:\n",
    "        response = requests.post(url, json=response_data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['response'], 200\n",
    "        else:\n",
    "            print(\"Error:\", response.status_code, response.text)\n",
    "            return {\"error\": f\"Error {response.status_code}: {response.text}\"}, response.status_code\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return {\"error\": f\"An error occurred: {e}\"}, 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a1a2a",
   "metadata": {},
   "source": [
    "### Uso para completamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1bb2d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: The capital of France is Paris.\n",
      "Codigo de respuesta: 200\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "model = \"llama3.2\"\n",
    "\n",
    "response = generate(model, prompt)\n",
    "print(\"Respuesta:\",response[0])\n",
    "print(\"Codigo de respuesta:\", response[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228a831",
   "metadata": {},
   "source": [
    "### Uso para completamiento de texto con rúbrica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f80ad0",
   "metadata": {},
   "source": [
    "**Nota**: Para el apartado format Utiliza JSON schema para eso.\n",
    "Links: \n",
    "- https://tour.json-schema.org/\n",
    "- https://json-schema.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eddae32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: { \"age\": 22, \"available\" : true }\n",
      "Codigo de respuesta: 200\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ollama is 22 years old and is busy saving the world. Respond using JSON\"\n",
    "model = \"llama3.2\"\n",
    "format = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"age\": {\n",
    "        \"type\": \"integer\"\n",
    "      },\n",
    "      \"available\": {\n",
    "        \"type\": \"boolean\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\n",
    "      \"age\",\n",
    "      \"available\"\n",
    "    ]\n",
    "  }\n",
    "\n",
    "response = generate(model, prompt, format)\n",
    "print(\"Respuesta:\",response[0])\n",
    "print(\"Codigo de respuesta:\", response[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
